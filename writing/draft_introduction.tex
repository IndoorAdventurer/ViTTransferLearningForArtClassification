\section{Introduction}
In the field of \textit{Computer Vision} (CV), \textit{Convolutional Neural Networks} (CNNs) have played an important role since the introduction of AlexNet, roughly a decade ago [BRON]. Recently, however, a new type of \textit{Artificial Neural Network} (ANN), called \textit{Vision Transformer} (ViT), has gained state of the art performance on common learning benchmarks, including the famous ImageNet dataset [BRON/VOETNOOT].

Exciting as this may be, a plain ViT is not likely to become useful in circumstances with limited training data and/or only consumer-grade hardware available for learning. This is due to intrinsic limitations within the ViT architecture, that will be elaborated on below [subsectie]. The current paper investigates whether utilizing \textit{Transfer Learning} (TL) capabilities of ViTs can help overcome these limitations, and if this allows ViTs to become the preferred architecture in the aforementioned circumstances as well.

This introduction section will first give high level descriptions of CNNs and ViTs, and illustrate how they compare to one another. What follows, is an explanation of TL, and why it might help ViTs in circumstances with limited data/compute available. Finally, background information on the topic of TL using ViTs is given, and a research question is proposed.

\subsection{Convolutional Neural Networks}
\paragraph{Architecture}
The modern CNN architecture is often attributed to [BRON]. It has asymptotically less trainable parameters per layer than the classical \textit{Multilayer Perceptron} (MLP) [BRON], and achieves this reduction in ways that make sense for the topology of an image. Firstly, connections are restricted to a neuron its so called \textit{local receptive field}. This is a small patch of neurons in the previous layer, with similar spatial location. The top right of figure [FIGREF] shows an example. Here, one neuron and its receptive field are highlighted. The top left shows a hypothetical MLP implementation. Note how it has many more connections overall.

Secondly, instead of using a different set of trainable weights to connect a neuron to its receptive field, the same set, called a \textit{convolution kernel}, is shared. In figure [FIGREF] the 4 accentuated connections correspond to one such kernel. The other shown connections don't add to the number of trainable weights, as they use this kernel as well. Essentially, a one-layered mini-MLP is slided over the input grid to produce the output grid -- which is called a \textit{feature map}. In reality, multiple kernels and feature maps exist per layer.

Finally, to reduce the size even further, so called \textit{pooling layers} are added. These reduce the spatial resolution of a feature map by aggregating patches of neurons into one, using $max(\cdot)$, or $mean(\cdot)$, for example.

\paragraph{Properties}
The architecture as described, is largely invariant to shifts and small distortions of the input image. More important, is that CNNs can be seen as automatic feature extractors. It is well documented (see, for example, [REFS]) that the first layers of a CNN detect low level features, such as edges and corners. These are combined in subsequent layers to detect higher level features. The final layer then (hopefully) represents the input in an abstract manner, such that, for example, an MLP-type layer can extract all the information it needs from it to perform the task at hand.

\subsection{Vision Transformers}
\paragraph{Architecture}
Where a CNN reduces the number of trainable weights by taking an image's topology into account, a ViT does this by learning itself what information it should incorporate in successive representations of the input. As such, it is not restricted to a local receptive field, but can potentially include information from all over the previous layer.

ViTs were first introduced in [BRON], and are based on the \textit{Transformer} architecture [BRON]. This type of ANN was originally designed to solve problems in \textit{natural language processing} (a field it now dominates), and takes a variable length sequence of \textit{token embeddings}\footnote{The model has some vocabulary of tokens it knows, for example all words in the English language. For each it learns a vector representation, called a token embedding.} as input. While Transformers are made up of an \textit{encoder} and \textit{decoder}, the latter won't be discussed here, as it is discarded in the ViT architecture.

Central to the encoder is the \textit{self-attention mechanism}. The \textit{attention heads} that implement it, start with 3 learned projection matrices.

%The architecture consists of an \textit{encoder}, and \textit{decoder}, but since the latter is discarded in ViTs, it won't be discussed here.

%Before entering the encoder, each token is converted into a corresponding vector representation, called a \textit{token embedding}. This is done using a learned mapping from tokens to vectors. To make the network aware of a token's location within the sequence, positional encodings are added to the embeddings as well\footnote{Trigonometric functions are used for this, instead of learned encodings.}.

%The encoder itself is composed of a stack of \textit{encoder blocks}. The first block gets the embeddings as input; the rest the output of its predecessor. Each block contains two components, namely a \textit{multiheaded self-attention} layer, which will be discussed below, and a simple MLP that processes all positions (i.e. vectors) of the input sequence individually. Residual connections add a component's output to its input, and this sum is then normalized; i.e. $Normalize(x + Component(x))$\footnote{Note that this means the dimensions of a component's output are identical to that of its input.}.

%Checklist:
% - constant number of operations to relate tokens, so regardless of distance
% - weight sharing because same learned operations are applied to each (each token goes through the same pipeline)

\paragraph{Self-attention}
TODO

\paragraph{From Transformer to ViT}
TODO

% While a CNN is restricted to only incorporate information from its surrounding pixels in a subsequent layer, a ViT 

\subsection{A limitation of Vision Transformers}
While ViTs are promising; the fact that an image's topology is not incorporated in the architecture, makes that they lack a so called image specific \textit{Inductive Bias}, which CNNs do have. Intuitively this means a ViT first has to learn what an image is, before it can move on to the learning task at hand. The result is that they require substantially more training data (and time) than CNNs do. 
% Bronnen waaruit dit blijkt?
TODO

% CNNs have image specific inductive bias: the architecture itself is tailored to 2D images, and as such has structural information about how images work built-in. This is not the case for ViTs, which lack an image specific inductive bias, which makes them very data hungry.

\subsection{Transfer Learning}
TODO

\subsection{The current study}
TODO
