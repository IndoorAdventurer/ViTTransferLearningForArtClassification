%---DATASET-------------------------------------------------------------------
% Describes the dataset that I am using:
@INPROCEEDINGS{mensink14icmr,
  author = {Thomas Mensink and Jan van Gemert},
  title = {The Rijksmuseum Challenge: Museum-Centered Visual Recognition},
  booktitle = {ACM International Conference on Multimedia Retrieval (ICMR)},
  year = {2014}
}

%---TRANSFER-LEARNING---------------------------------------------------------
% PHD Thesis of Sabatelli:
@phdthesis{sabatelli2022contributions,
  title={Contributions to Deep Transfer Learning: from Supervised to Reinforcement Learning},
  author={Sabatelli, Matthia},
  year={2022},
  school={Universit{\'e} de Li{\`e}ge,​ Li{\`e}ge,​​ Belgique}
}

% The paper by Sabatelli that I will `replicate':
@inproceedings{sabatelli2018deep,
  title={Deep transfer learning for art classification problems},
  author={Sabatelli, Matthia and Kestemont, Mike and Daelemans, Walter and Geurts, Pierre},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  pages={0--0},
  year={2018}
}

% Paper from long ago that found off-the-shelf performance of CNN was better
% than from-scratch performance of non-CNN models:
@inproceedings{sharif2014cnn,
  title={CNN features off-the-shelf: an astounding baseline for recognition},
  author={Sharif Razavian, Ali and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={806--813},
  year={2014}
}

% Paper that did lots of quantitative experiments w.r.t. CNNs transfer learning
% abilities:
@article{yosinski2014transferable,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

%---VITS----------------------------------------------------------------------
% Paper that introduced Transformers:
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% Paper that introduced ViTs:
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

%---CNNS----------------------------------------------------------------------
% Paper that modern CNNs are attributed to
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

% AlexNet paper: start of CNN dominance
@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

%---TRANSFER-LEARNING-WITH-VITS-----------------------------------------------
% Most interesting paper in the category:
@article{matsoukas2021time,
  title={Is it time to replace cnns with transformers for medical images?},
  author={Matsoukas, Christos and Haslum, Johan Fredin and S{\"o}derberg, Magnus and Smith, Kevin},
  journal={arXiv preprint arXiv:2108.09038},
  year={2021}
}

% Says interesting stuff about off-the-shelf performance of ViTs
@inproceedings{zhou2021convnets,
  title={ConvNets vs. Transformers: Whose Visual Representations are More Transferable?},
  author={Zhou, Hong-Yu and Lu, Chixiang and Yang, Sibei and Yu, Yizhou},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2230--2238},
  year={2021}
}

% This one did much quantitative analysis on ViTs
@article{raghu2021vision,
  title={Do vision transformers see like convolutional neural networks?},
  author={Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
