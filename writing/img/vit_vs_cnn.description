The intuition behind CNNs and ViTs. The first row shows a hypothetical MLP to handle 2D images (left), and how a CNN improves on this situation (right). Similarly, the second row shows how an MLP (left) and a transformer's encoder block (right) would handle a series of token vectors. Note that illustrations are not entirely true to reality, as important details were left out for clarity.
