\begin{abstract} {Convolutional Neural Networks (CNNs) have become the de facto standard in Computer Vision, and played a major role in the advancement of this field. In recent years, however, Vision Transformer based models (VTs) have been outperforming CNNs across the board. While this is exciting, CNNs still have an advantage on small datasets, due to their so-called image specific inductive bias. Recent work suggests that transfer learning methods allow VTs to compete with CNNs on some of these small datasets. This thesis investigates whether that also holds true for art classification problems within the digital humanities. To this end, it compares popular VTs and CNNs in terms of their off-the-shelf and fine-tuning transferability, when going from \textit{ImageNet1K} to target tasks provided by the \textit{Rijksmuseum Challenge} dataset. The results show that VTs posses superior off-the-shelf feature extraction capabilities here, and that in general, transfer learning allows VTs to become an interesting alternative to consider within the digital humanities.\vspace{8ex}}
\end{abstract}
